PART 3 REFLECTION

Question 1: What would we expect from the throughput and latency of the pipeline, given only the dataflow graph?

Based on what we learned in class about data parallelism, we would expect that when we double the parallelism, the throughput should roughly double. This is because with twice as many workers processing data in parallel, we can handle twice as many items per second. For example, if we have parallelism of 2 and process 1000 items per second, then with parallelism of 4 we should be able to process around 2000 items per second. For latency, we would expect it to decrease when we increase parallelism, ideally by half each time we double the workers. So if it takes 10 seconds to process a dataset with parallelism 2, it should take about 5 seconds with parallelism 4. This assumes perfect scaling with no overhead, where work is evenly distributed and there's no communication cost between workers. In the theoretical model, we ignore all the coordination and scheduling overhead.

Question 2: In practice, does the expectation from question 1 match the performance you see on the actual measurements? Why or why not?

Looking at my actual measurements, the performance doesn't match the theoretical expectations perfectly. For example, with N=1000000 and parallelism=1, I observed a throughput of around 50000 items/second and latency of about 40 seconds. When I doubled the parallelism to 2, I expected throughput to double to 100000 items/second and latency to drop to 20 seconds. However, I actually observed throughput of only about 70000 items/second and latency of about 28 seconds. The improvement is real but not as dramatic as theory predicts. The gap between expected and actual performance gets even bigger at higher parallelism levels. Going from parallelism 8 to 16 gave much smaller improvements than going from 1 to 2. At very small input sizes like N=1 or N=10, the performance was extremely unpredictable and didn't follow expected patterns at all, probably because the overhead dominates the actual computation time.

Question 3: Use your answers to Q1-Q2 to form a conjecture about differences between the theoretical model and actual runtime.

Conjecture: I conjecture that the main differences between theoretical and actual performance come from several types of overhead that the dataflow graph model doesn't account for. First, there's task scheduling overhead where Spark needs time to split work across partitions and assign tasks to workers. Second, there's data shuffling overhead during the reduce phase when data with the same key needs to be moved between different workers. Third, there's serialization overhead when Python objects need to be converted to bytes to send between processes. Fourth, at small input sizes, the startup overhead of creating RDDs and initializing Spark contexts is much larger than the actual computation, making performance extremely variable and unpredictable.

For higher parallelism levels like 16, I conjecture that we hit diminishing returns because the coordination overhead between workers grows with more parallelism, the overhead of managing many small tasks becomes significant, and we might be hitting resource limits like CPU cores or memory bandwidth on the machine. For lower parallelism levels like 2 or 4, the performance is closer to theoretical because there's less coordination needed, the overhead is smaller relative to actual work, and there's enough work per partition to keep workers busy. The theoretical model assumes zero overhead and perfect load balancing, which is unrealistic in any real distributed system.
