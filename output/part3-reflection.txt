PART 3 REFLECTION

Question 1:
From the dataflow graph we would normally expect throughput to increase as we use more data parallelism and latency to decrease. If we double the number of partitions then, in the ideal model, throughput should roughly double because twice as many workers are processing data at the same time. For a fixed input size we would also expect the latency to be cut about in half when we double the workers, since the work is split evenly across more partitions. This picture assumes that there is perfect load balancing, so every worker gets the same amount of work and none of them are idle. It also assumes that there is no overhead from communication, scheduling, or coordination between workers. In short, the theoretical model predicts clean linear speedup in throughput and inverse-linear decrease in latency as parallelism increases.

Question 2:
When we look at the actual measurements, the performance does not match the perfect scaling from Question 1. For example, going from P=1 to P=2 for a large input like N=1,000,000 increased throughput by less than a factor of 2 and reduced latency by less than one half. The improvements get even smaller when we go from P=4 to P=8 or P=8 to P=16, and sometimes the runtime even gets worse at very high parallelism levels. For small inputs such as N=1 or N=10, the results are very noisy and do not follow any clear pattern because the overhead dominates the useful work. Overall, the real system shows diminishing returns: we get some speedup from extra partitions at first, but the gains flatten out and can even reverse.

Question 3:
These differences suggest that the real system has extra costs that are not captured by the simple dataflow-graph model. Spark needs time to schedule tasks for each partition, and this scheduling overhead is paid even when the partitions are very small. There is also communication and shuffling overhead when data with the same key must be moved between workers during reduce-style stages. Serialization and deserialization of Python objects add more overhead, especially when many small tasks are created. At very high parallelism levels the workers also start competing for limited hardware resources such as CPU cores and memory bandwidth, which blocks perfect scaling. All of these effects break the assumption of zero overhead and perfect load balancing that underlies the theoretical expectations.

Conjecture:
I conjecture that real execution is limited by scheduling overhead, communication and serialization costs, and hardware contention between workers, so throughput and latency cannot scale linearly with the number of partitions as predicted by the idealized dataflow model.
