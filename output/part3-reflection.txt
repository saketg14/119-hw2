PART 3 REFLECTION

Question 1: Based on what we learned in class about data parallelism, I would expect throughput to roughly double whenever we double the parallelism. With twice as many workers running in parallel, the system should process about twice as many items per second. Latency for a fixed-size dataset should decrease as parallelism increases and ideally drop by about half whenever the number of workers doubles. This expectation assumes perfect load balancing where each worker receives an equal amount of work without idle time. In the theoretical model we also assume zero overhead for communication, scheduling, or coordination, which makes the scaling appear much more ideal than what we see in real systems.

Question 2: In practice, the performance does not match the theoretical expectations exactly. For example, with N=1000000 and parallelism=1, I observed a throughput of around 50000 items per second and a latency of about 40 seconds. When increasing parallelism to 2, I expected throughput to double and latency to halve, but instead throughput increased only to around 70000 items per second and latency dropped only to around 28 seconds. The improvements become even smaller at higher levels of parallelism, such as going from 8 to 16 workers. At extremely small input sizes like N=1 or N=10, overhead dominates completely and performance becomes unpredictable because the actual computation is negligible compared to startup and scheduling costs.

Question 3: Using the differences between Q1 and Q2, I conjecture that the main reason actual performance diverges from the theoretical model is the existence of overheads that are ignored in the dataflow graph abstraction. Spark incurs task scheduling overhead when dividing work across workers, communication overhead when shuffling data during reduce stages, and serialization overhead when converting Python objects into bytes for transport. These costs accumulate and become especially visible as parallelism increases. At very small input sizes, startup overhead and Spark initialization dominate the total runtime, making performance inconsistent. At high parallelism levels such as 16, diminishing returns occur because coordination overhead becomes large, tasks become too small, and hardware limits such as CPU cores and memory bandwidth begin to constrain performance. The theoretical model assumes perfect scaling with zero overhead, which does not reflect the behavior of real distributed systems.
